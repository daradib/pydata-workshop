{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Modeling  With Python#\n",
    "### Models for Continuous and Categorical Outcomes\n",
    "\n",
    "This material uses Python to demonstrate some aspects of statistical models with continuous values to be predicted.\n",
    "\n",
    "* If the dependent variable (the variable we are trying to explain or predict) is continuous (has a large range, like price of housing), then we use **multiple regression**.\n",
    "\n",
    "* If the dependent variable is categorical (or represents a choice outcome) with two values (rent or own for example) than a **binary logit model** or **logistic regression** is usually the preferred model.\n",
    "\n",
    "* if the dependent variable is categorical (or represents a choice outcome) with a small number of values (like travel modes), then the most common model form is **Multinomial Logit (MNL)**\n",
    "\n",
    "In today's session we introduce Python libraries that enable estimating models the multiple regression type.  Theory is only briefly reviewed, as these methods should be supported by a full course (or more).  The material is really meant to help students who have been exposed to these methods using another platform such as Stata, R, SAS, or SPSS, and want to be able to use Python to undertake their statistical model building."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "Simple linear regression is an approach for predicting a **quantitative response** using a **single feature** (or \"predictor\" or \"input variable\"). It takes the following form:\n",
    "\n",
    "$Y = \\beta_0 + \\beta_1x + \\epsilon$, where $\\epsilon\\sim N\\left(0,\\sigma^{2}\\right)$\n",
    "\n",
    "- $y$ is the dependent variable - the one we are trying to explain or predict\n",
    "- $x$ is the independent or explanatory variable that we are using to help explain or predict the value of $y$ with\n",
    "- $\\beta_0$ is the intercept\n",
    "- $\\beta_1$ is the coefficient for x, which is the slope of the line that minimizes the sum of the squared errors\n",
    "- $\\epsilon$ is the error term, assumed to be normally distributed with mean of zero and variance of $\\sigma^{2}$\n",
    "\n",
    "Together, $\\beta_0$ and $\\beta_1$ are called the **model coefficients**. To create your model, you must \"learn\" or \"estimate\" the values of these coefficients. And once we've learned these coefficients, we can use the model to predict values of $y$ based on new values of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Model Coefficients\n",
    "\n",
    "Generally speaking, coefficients are estimated using the **least squares criterion**, which means we are find the line (mathematically) which minimizes the **sum of squared residuals** (or \"sum of squared errors\"):\n",
    "\n",
    "In the figure below:\n",
    "- The black dots are the **observed values** of x and y.\n",
    "- The red line is our **least squares line**.\n",
    "- The **residuals** are the vertical distances between the observed values and the least squares line.\n",
    "\n",
    "How do the model coefficients relate to the least squares line?\n",
    "- $\\beta_0$ is the **intercept** (the value of $y$ when $x$=0)\n",
    "- $\\beta_1$ is the **slope** (the change in $y$ divided by change in $x$)\n",
    "\n",
    "Here is a graphical depiction of those calculations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"regression.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Startup steps\n",
    "import pandas as pd, numpy as np, statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt, matplotlib.cm as cm, matplotlib.font_manager as fm\n",
    "import matplotlib.mlab as mlab\n",
    "import time, requests\n",
    "from scipy.stats import pearsonr, ttest_rel\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import folium\n",
    "from folium import plugins\n",
    "from folium.plugins import MarkerCluster\n",
    "from folium.map import FeatureGroup, Icon, Layer, Marker\n",
    "from folium.plugins.measure_control import MeasureControl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by generating some synthetic data that is generated using an equation for which we supply the parameters.  It enables us to verify that the model estimation code is correctly 'learning' the correct parameters, before we use it on real data. \n",
    "\n",
    "Below we generate 100 values of Y from an equation: $Y = \\beta_0 + \\beta_1 x + \\epsilon$.  \n",
    "\n",
    "We set $\\beta_0 = 0$ and $\\beta_1 = 2$ and draw values of $\\epsilon$ from a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nsample = 300\n",
    "x = np.linspace(0, 10, nsample)\n",
    "beta = np.array([0, 2])\n",
    "e = np.random.normal(size=nsample)*2\n",
    "X = sm.add_constant(x)\n",
    "y = np.dot(X, beta) + e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data and the model.  Note that the intercept is set to zero in this example initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(10,8), )\n",
    "plt.plot([0, 10], [0, 20])\n",
    "plt.scatter(x, y, marker=0, s=10, c='g')\n",
    "plt.axis([0, 10, 0, 20])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we 'fit' the model to the data, which means we compute the values of $\\beta_0$ and $\\beta_1$ that minimize the sum of the squared errors.  We use Statsmodels for fitting the model.  There are two different syntax styles to use for this.  The first one uses an X matrix and a y array, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract any of the results from the fitted model we want, since the fitted model is a Python object and has attributes we can interrogate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Parameters: ', results.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statsmodels calculates 95% confidence intervals for our model coefficients, which are interpreted as follows: If the population from which this sample was drawn was **sampled 100 times**, approximately **95 of those confidence intervals** would contain the \"true\" coefficient.\n",
    "\n",
    "To get the 95% confidence interval around $\\beta_0$ and $\\beta_1$ we can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.conf_int()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing and p-values\n",
    "\n",
    "Closely related to confidence intervals is **hypothesis testing**. Generally speaking, you start with a **null hypothesis** and an **alternative hypothesis** (that is opposite the null). Then, you check whether the data supports **rejecting the null hypothesis** or **failing to reject the null hypothesis**.\n",
    "\n",
    "(Note that \"failing to reject\" the null is not the same as \"accepting\" the null hypothesis. The alternative hypothesis may indeed be true, except that you just don't have enough data to show that.)\n",
    "\n",
    "As it relates to model coefficients, here is the conventional hypothesis test:\n",
    "- **null hypothesis:** There is no relationship between $x$ and $y$ (and thus $\\beta_1$ equals zero)\n",
    "- **alternative hypothesis:** There is a relationship between $x$ and $y$ (and thus $\\beta_1$ is not equal to zero)\n",
    "\n",
    "How do we test this hypothesis? Intuitively, we reject the null (and thus believe the alternative) if the 95% confidence interval **does not include zero**. Conversely, the **p-value** represents the probability that the coefficient is actually zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.pvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the 95% confidence interval **includes zero**, the p-value for that coefficient will be **greater than 0.05**. If the 95% confidence interval **does not include zero**, the p-value will be **less than 0.05**. Thus, a p-value less than 0.05 is one way to decide whether there is likely a relationship between the feature and the response. (Again, using 0.05 as the cutoff is just a convention.)\n",
    "\n",
    "In this case, the p-value for $x$ is far less than 0.05, and so we **believe** that there is a relationship between $x$ and $y$.\n",
    "\n",
    "Note that we generally ignore the p-value for the intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Well Does the Model Fit the data?\n",
    "\n",
    "The most common way to evaluate the overall fit of a linear model is by the **R-squared** value. R-squared is the **proportion of variance explained**, meaning the proportion of variance in the observed data that is explained by the model, or the reduction in error over the **null model**. (The null model just predicts the mean of the observed response, and thus it has an intercept and no slope.)\n",
    "\n",
    "R-squared is between 0 and 1, and higher is better because it means that more variance is explained by the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('R2: ', results.rsquared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect the residuals (the errors) computed by comparing the predicted values of y to the observed ones.  The mean of the residuals should be zero and they should be normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.resid.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "plt.rcParams['figure.figsize']=8,8\n",
    "#plot(residuals.mean(),0, residuals.mean(), 2.25)\n",
    "sns.set_style(\"white\")\n",
    "sns.set_style(\"ticks\")\n",
    "ax = sns.distplot(results.resid, fit=norm, kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "Simple linear regression can easily be extended to include multiple features. This is called **multiple linear regression**:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n + \\epsilon$\n",
    "\n",
    "Let's add an additional x variable to our synthetic model, using x squared, and fit it again with Statsmodels.\n",
    "\n",
    "We generate new values of $y$ using $\\beta_0 = 0$, $\\beta_1 = 2$ and $\\beta_2 = 0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nsample = 300\n",
    "x = np.linspace(0, 10, 300)\n",
    "X = np.column_stack((x, x**2))\n",
    "beta = np.array([1, 2, .5])\n",
    "e = np.random.normal(size=nsample)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = sm.add_constant(X)\n",
    "y = np.dot(X, beta) + e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.regplot(x=X[:,1], y=y, scatter_kws={\"s\": 10}, order=2, ci=None, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.resid.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "plt.rcParams['figure.figsize']=8,8\n",
    "sns.set_style(\"white\")\n",
    "sns.set_style(\"ticks\")\n",
    "ax = sns.distplot(results.resid, fit=norm, kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**R-squared will always increase as you add more features to the model**, even if they are unrelated to the response. Thus, selecting the model with the highest R-squared is not a reliable approach for choosing the best linear model.\n",
    "\n",
    "There is alternative to R-squared called **adjusted R-squared** that penalizes model complexity (to control for overfitting), but it generally [under-penalizes complexity](http://scott.fortmann-roe.com/docs/MeasuringError.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Estimating a Multiple Regression on Housing Prices (Hedonic Regression)##\n",
    "\n",
    "Now let's use real data.  We pull some housing sales transactions from Redfin for this, for one month of sales in San Francisco, looking back from March 5th 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf0 = pd.read_csv('data/redfin_2017-03-05-17-45-34-san-francisco-county-1-month.csv')\n",
    "sf0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = sf0.rename(index=str, columns={'SALE TYPE': 'saletype',\n",
    "    'SOLD DATE': 'solddate', 'PROPERTY TYPE': 'proptype', 'ADDRESS': 'address',\n",
    "    'CITY': 'city', 'STATE': 'state', 'ZIP': 'zip', 'PRICE': 'price', 'BEDS': 'beds',\n",
    "    'BATHS': 'baths', 'LOCATION': 'location', 'SQUARE FEET': 'sqft', 'LOT SIZE': 'lotsize',\n",
    "    'YEAR BUILT': 'yrbuilt', 'DAYS ON MARKET': 'daysonmkt', '$/SQUARE FEET': 'pricesqft',\n",
    "    'LATITUDE': 'latitude', 'LONGITUDE': 'longitude', 'HOA/MONTH': 'hoamonth',\n",
    "    'URL (SEE http://www.redfin.com/buy-a-home/comparative-market-analysis FOR INFO ON PRICING)': 'url',\n",
    "    'STATUS': 'status', 'NEXT OPEN HOUSE START TIME': 'nextopenstart', 'NEXT OPEN HOUSE END TIME': 'nextopenend',\n",
    "    'SOURCE': 'source', 'MLS#': 'mls', 'FAVORITE': 'favorite', 'INTERESTED': 'interested'\n",
    "    })\n",
    "\n",
    "sf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some additional census data to merge on to our sales transactions.  But we don't have a census block on our records.... what to do?  We can go to our trusty FCC geocoder API, of course!\n",
    "\n",
    "**In the interest of time (it takes a couple of minutes to run), you can skip the next three cells and just start from the cell that loads the data from a csv 4 cells below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fips(row):\n",
    "    time.sleep(pause)\n",
    "    url = 'http://geo.fcc.gov/api/census/block/find?format=json&latitude={}&longitude={}'\n",
    "    request = url.format(row['latitude'], row['longitude'])\n",
    "    response = requests.get(request)\n",
    "    data = response.json()\n",
    "    \n",
    "    # return multiple values as a series - this will create a dataframe with multiple columns\n",
    "    return pd.Series({'fips_code':data['Block']['FIPS'], 'county':data['County']['name']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "pause = 0.01\n",
    "fips = sf.apply(get_fips, axis=1)\n",
    "sf = pd.concat([sf, fips], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sf.to_csv('data/sf1_blk.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start from here if you want to skip the geocoding step...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = pd.read_csv('data/sf1_blk.csv', usecols=['proptype', 'price', 'beds', 'baths', 'sqft', 'lotsize',\\\n",
    "    'yrbuilt', 'hoamonth', 'fips_code', 'latitude', 'longitude'], converters={'fips_code': str})\n",
    "sf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv = pd.read_csv('data/smallvars.csv', converters={'block_id': str})\n",
    "sv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf1 = pd.merge(sf, sv, left_on='fips_code', right_on='block_id', how='inner')\n",
    "sf1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g = sns.jointplot(\"sqft\", \"price\", data=sf1, kind=\"reg\", scatter_kws={\"s\": 10}, size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit a simple linear regression of price on sqft, since there is clearly a strong relationship between them.\n",
    "\n",
    "We will use Statsmodels for this as before, but now we use the Patsy syntax to specify the model.  Patsy uses the same format as the R language for specifying models.  The link below provides good documentation on the format.\n",
    "\n",
    "http://www.statsmodels.org/dev/example_formulas.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from patsy import dmatrices\n",
    "y, X = dmatrices('price ~ sqft', \n",
    "                 data=sf1, return_type='dataframe')\n",
    "mod = sm.OLS(y, X)\n",
    "res = mod.fit()\n",
    "residuals = res.resid\n",
    "predicted = res.fittedvalues\n",
    "observed = y\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you intepret the R-squared value?\n",
    "\n",
    "Check-in time: http://bitly.com/cp255\n",
    "How can you interpret the coefficient on sqft?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common thing to explore is whether a transformation of the dependent and/or independent variables help improve the degree to which the relationship is linear, and the fit of the model.  Most models of housing prices, or 'hedonic price models' are specified with the log of price, and often the log of continuous variables like sqft.  Let's look at the relationship once we log-transform both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g = sns.jointplot(np.log(sf1['sqft']), np.log(sf1['price']), kind=\"reg\", scatter_kws={\"s\": 8}, size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's re-estimate the model using the log-log transformation of price and sqft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y, X = dmatrices('np.log(price) ~ np.log(sqft)', \n",
    "                 data=sf1, return_type='dataframe')\n",
    "mod = sm.OLS(y, X)\n",
    "res = mod.fit()\n",
    "residuals = res.resid\n",
    "predicted = res.fittedvalues\n",
    "observed = y\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model fit improves considerably.  How do you interpret the coefficient for sqft now?\n",
    "\n",
    "Next let's add the number of baths and see how that changes the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y, X = dmatrices('np.log(price) ~ np.log(sqft) + baths', \n",
    "                 data=sf1, return_type='dataframe')\n",
    "mod = sm.OLS(y, X)\n",
    "res = mod.fit()\n",
    "residuals = res.resid\n",
    "predicted = res.fittedvalues\n",
    "observed = y\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add bedrooms now and see how that changes the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y, X = dmatrices('np.log(price) ~ np.log(sqft) + baths + beds', \n",
    "                 data=sf1, return_type='dataframe')\n",
    "mod = sm.OLS(y, X)\n",
    "res = mod.fit()\n",
    "residuals = res.resid\n",
    "predicted = res.fittedvalues\n",
    "observed = y\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm.  Expected that result on bedrooms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y, X = dmatrices('np.log(price) ~ np.log(sqft) + baths + beds + np.log(lotsize)', \n",
    "                 data=sf1, return_type='dataframe')\n",
    "mod = sm.OLS(y, X)\n",
    "res = mod.fit()\n",
    "residuals = res.resid\n",
    "predicted = res.fittedvalues\n",
    "observed = y\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm... the R-Squared went down when we added a variable?  That's strange.  What happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sf1.proptype.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "single = sf1[sf1['proptype']=='Single Family Residential'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y, X = dmatrices('np.log(price) ~ np.log(sqft) + baths + beds + np.log(lotsize) ', \n",
    "                 data=single, return_type='dataframe')\n",
    "mod = sm.OLS(y, X)\n",
    "res = mod.fit()\n",
    "residuals = res.resid\n",
    "predicted = res.fittedvalues\n",
    "observed = y\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "condo = sf1[sf1['proptype']=='Condo/Co-op'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y, X = dmatrices('np.log(price) ~ np.log(sqft) + baths + beds ', \n",
    "                 data=condo, return_type='dataframe')\n",
    "mod = sm.OLS(y, X)\n",
    "res = mod.fit()\n",
    "residuals = res.resid\n",
    "predicted = res.fittedvalues\n",
    "observed = y\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding dummy variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y, X = dmatrices('np.log(price) ~ np.log(sqft) + baths + C(beds)  ', \n",
    "                 data=condo, return_type='dataframe')\n",
    "mod = sm.OLS(y, X)\n",
    "res = mod.fit()\n",
    "residuals = res.resid\n",
    "predicted = res.fittedvalues\n",
    "observed = y\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding Interaction Terms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y, X = dmatrices('np.log(price) ~ np.log(sqft) + baths + beds + beds * baths ', \n",
    "                 data=condo, return_type='dataframe')\n",
    "mod = sm.OLS(y, X)\n",
    "res = mod.fit()\n",
    "residuals = res.resid\n",
    "predicted = res.fittedvalues\n",
    "observed = y\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding neighborhood variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y, X = dmatrices('np.log(price) ~ np.log(sqft) + baths + beds  + bgmedinc +\\\n",
    "                 proprent + lnjobs5000m', \n",
    "                 data=condo, return_type='dataframe')\n",
    "mod = sm.OLS(y, X)\n",
    "res = mod.fit()\n",
    "residuals = res.resid\n",
    "predicted = res.fittedvalues\n",
    "observed = y\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = observed.join(predicted.to_frame(name='predicted'))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=8,8\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.set_style(\"ticks\")\n",
    "ax = sns.distplot(residuals, fit=norm, kde=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g = sns.jointplot(predicted, residuals, kind=\"reg\", scatter_kws={\"s\": 8}, size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.jointplot(\"np.log(price)\", \"predicted\", data=data, kind=\"reg\", scatter_kws={\"s\": 8}, size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to explore your data further to look at pairwise relationships, you can use the Seaborn PairGrid plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keepcols = [ 'price', 'beds', 'baths', 'sqft', 'lotsize', 'yrbuilt']\n",
    "condo_small=condo[keepcols]\n",
    "condo_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g = sns.PairGrid(condo_small)\n",
    "g.map(plt.scatter);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn\n",
    "\n",
    "* Try experimenting with the model for single family or condo sales to see if you can improve it significantly using the available variables.\n",
    "\n",
    "* If you have time and energy, you could try grabbing your own Redfin sales transactions for a place you are interested in. But keep in mind you'll need to add locational context variables on your own.  Census data is pretty easy, but accessibility variables require more work, using pandana and/or urbanaccess, and getting your own point of interest (POI) data to compute accessibility to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
